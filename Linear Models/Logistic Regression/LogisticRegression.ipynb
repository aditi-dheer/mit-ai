{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem is a binary classification problem. We are trying to predict whether a customer will leave their current telecom company or whether a customer will not leave the company. This problem is well suited for a Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Build DataFrame and Define ML Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "We will work with a data set called \"cell2celltrain.\" This data set is already pre-processed, with the proper formatting, outliers and missing values taken care of, and all numerical columns scaled to the [0, 1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"..\", \"..\", \"data\", \"cell2celltrain.csv\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Label\n",
    "\n",
    "This is a binary classification problem in which we will predict customer churn. The label is the `Churn` column. The label will either have the value `True` or `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Features\n",
    "\n",
    "To implement a Logistic Regression model, we must use only the numeric columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c83bb20720827de690d7e68b5486396e",
     "grade": false,
     "grade_id": "cell-features",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "feature_list_df = df.select_dtypes(include='float64')\n",
    "feature_list = [col for col in feature_list_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Labeled Examples from the Data Set \n",
    "\n",
    "Let's obtain columns from our data set to create labeled examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5bbfd2d757cc6a47662581c9217320d0",
     "grade": false,
     "grade_id": "cell-XY",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 51047\n",
      "\n",
      "Number of Features: 35\n",
      "['MonthlyRevenue', 'MonthlyMinutes', 'TotalRecurringCharge', 'DirectorAssistedCalls', 'OverageMinutes', 'RoamingCalls', 'PercChangeMinutes', 'PercChangeRevenues', 'DroppedCalls', 'BlockedCalls', 'UnansweredCalls', 'CustomerCareCalls', 'ThreewayCalls', 'ReceivedCalls', 'OutboundCalls', 'InboundCalls', 'PeakCallsInOut', 'OffPeakCallsInOut', 'DroppedBlockedCalls', 'CallForwardingCalls', 'CallWaitingCalls', 'MonthsInService', 'UniqueSubs', 'ActiveSubs', 'Handsets', 'HandsetModels', 'CurrentEquipmentDays', 'AgeHH1', 'AgeHH2', 'RetentionCalls', 'RetentionOffersAccepted', 'ReferralsMadeBySubscriber', 'IncomeGroup', 'AdjustmentsToCreditRating', 'HandsetPrice']\n"
     ]
    }
   ],
   "source": [
    "y = df['Churn']\n",
    "X = feature_list_df\n",
    "\n",
    "print(\"Number of examples: \" + str(X.shape[0]))\n",
    "print(\"\\nNumber of Features: \" + str(X.shape[1]))\n",
    "print(str(list(X.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9e9e6658863fc8bd10d4689be2798529",
     "grade": false,
     "grade_id": "cell-trainingData",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16846, 35)\n",
      "(34201, 35)\n",
      "(16846,)\n",
      "(34201,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train a Logistic Regression Classification Model and Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below contains code that must be completed to train a Logistic Regression classification model, analyze its performance and print the results. The code below will train a Logistic Regression model on the training data, test the resulting model on the test data, and compute and return (1) the log loss of the resulting probability predictions on the test data and (2) the accuracy score of the resulting predicted class labels on the test data.\n",
    "\n",
    "*Note*: It is worth noting that evaluating a model’s training loss and evaluating a model’s accuracy is different. Accuracy measures what fraction of the examples are correctly predicted by the classifier, while training loss measures the average prediction error per training example over all training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "19ffaae4e364cd63985fe6c1fc30fb00",
     "grade": false,
     "grade_id": "cell-LR",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Prediction Probabilities: \n",
      " Class: False  Class: True\n",
      "     0.745478     0.254522\n",
      "     0.658678     0.341322\n",
      "     0.724462     0.275538\n",
      "     0.848644     0.151356\n",
      "     0.749374     0.250626\n",
      "Log loss: 0.5878464111333068\n",
      "Class labels: [False False False False False]\n",
      "Accuracy: 0.7097233764691915\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# 1. Create the LogisticRegression model object below and assign to variable 'model'\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 2. Fit the model to the training data below\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions on the test data using the predict_proba() method and assign the \n",
    "# result to the variable 'probability_predictions' below\n",
    "probability_predictions = model.predict_proba(X_test)\n",
    "\n",
    "# print the first 5 probability class predictions\n",
    "df_print = pd.DataFrame(probability_predictions, columns = ['Class: False', 'Class: True'])\n",
    "print('Class Prediction Probabilities: \\n' + df_print[0:5].to_string(index=False))\n",
    "\n",
    "# 4. Compute the log loss on 'probability_predictions' and save the result to the variable\n",
    "# 'l_loss' below\n",
    "l_loss = log_loss(y_test, probability_predictions)\n",
    "print('Log loss: ' + str(l_loss))\n",
    "\n",
    "\n",
    "# 5. Make predictions on the test data using the predict() method and assign the result \n",
    "# to the variable 'class_label_predictions' below\n",
    "class_label_predictions = model.predict(X_test)\n",
    "\n",
    "# print the first 5 class label predictions \n",
    "print('Class labels: ' + str(class_label_predictions[0:5]))\n",
    "\n",
    "# 6.Compute the accuracy score on 'class_label_predictions' and save the result \n",
    "# to the variable 'acc_score' below\n",
    "acc_score = accuracy_score(y_test, class_label_predictions)\n",
    "print('Accuracy: ' + str(acc_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Thresholds: Map Probabilities to a Class Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the output of the code cell above.\n",
    "\n",
    "Note that the `predict_proba()` method returns two columns. As stated, the first column contains the probability that an unlabeled example belongs to class `False` and the second column contains the probability that an unlabeled example belongs to class `True`.\n",
    "\n",
    "The `predict()` method outputs the actual class label (`True` or `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "print(model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the probabilities map to labels in the table below. The table contains:\n",
    "* 3 unlabeled examples\n",
    "* the resulting class probability values from the logistic regression model's `predict_proba()` method\n",
    "* the corresponding class label from the same logistic regression model's `predict()` method. \n",
    "\n",
    "The probability that that unlabeled \"Example 1\" is of class `False` is 0.745386. The probability that unlabeled \"Example 1\" is of class `True` is 0.254614. The `predict()` method assigns \"Example 1\" the class label `False`.\n",
    "\n",
    "\n",
    "<table align=left>\n",
    "   <tr>\n",
    "    <th></th>\n",
    "    <th>Class: False</th>\n",
    "    <th>Class: True</th>\n",
    "    <th>Class Label</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <th>Example 1</th>\n",
    "    <th>0.745386</th>\n",
    "    <th>0.254614</th>\n",
    "    <th>False</th>\n",
    "    </tr>\n",
    "     <tr>\n",
    "    <th>Example 2</th>\n",
    "    <th>0.745386</th>\n",
    "    <th>0.254614</th>\n",
    "    <th>False</th>\n",
    "    </tr>    \n",
    "     <tr>\n",
    "    <th>Example 3</th>\n",
    "    <th>0.436033</th>\n",
    "    <th>0.563967</th>\n",
    "    <th>True</th>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the Scikit-learn `predict()` method assign a class label based on probability values? For binary classification, the method defaults to a 0.5 threshold. If the resulting probability for class 0 is greater than or equal to 0.5, the unlabeled example is given a label of `False` On the other hand, if the probability for class 0 is less than 0.5,  the unlabeled example is given a label of `True`.\n",
    "\n",
    "Sometimes we may want a different threshold. \n",
    "\n",
    "The function `computeAccuracy()` takes a threshold value as an argument. It does the following:\n",
    "\n",
    "1. Loops through the array `probability_predictions` (obtained from the Logistic Regression model above)\n",
    "    * It extracts the first column's probability \n",
    "    * It checks if that probability is greater than or equal to the threshold value.\n",
    "    * If so, it assigns a class label of `False`. Otherwise it assigns a class label of `True`.\n",
    "    * It saves the new class label to list `labels`.\n",
    "2. Computes the accuracy score by comparing the new class labels contained in list `labels` with the ground truth labels contained in `y_test`.\n",
    "3. Returns the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAccuracy(threshold_value):\n",
    "    \n",
    "    labels=[]\n",
    "    for p in probability_predictions[:,0]:\n",
    "        if p >= threshold_value:\n",
    "            labels.append(False)\n",
    "        else:\n",
    "            labels.append(True)\n",
    "    \n",
    "    acc_score = accuracy_score(y_test, labels)\n",
    "    return acc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below calls the `computeAccuracy()` function with a few different threshold values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold value 0.43: Accuracy 0.7115042146503621\n",
      "Threshold value 0.44: Accuracy 0.7106731568324824\n",
      "Threshold value 0.50: Accuracy 0.7097233764691915\n",
      "Threshold value 0.55: Accuracy 0.708298705924255\n",
      "Threshold value 0.67: Accuracy 0.6491155170366852\n",
      "Threshold value 0.75: Accuracy 0.4929360085480233\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.43, 0.44, 0.50, 0.55, 0.67, 0.75]\n",
    "for t in thresholds:\n",
    "    print(\"Threshold value {:.2f}: Accuracy {}\".format(t, str(computeAccuracy(t))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
